{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_wrappers.wrappers.context import ContextCausalLMWrapper\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOURS = [f'C{i}' for i in range(10)]\n",
    "STYLES = ['solid', 'dotted', 'dashed', 'dashdot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TOKEN = os.environ['HUGGINGFACE_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = 'gpt2-xl'\n",
    "# MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# MODEL = 'meta-llama/Llama-2-7b-hf'\n",
    "MODEL = 'google/gemma-7b'\n",
    "MODEL_CONFIGS = {\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'attn_implementation': 'eager',\n",
    "    'device_map': DEVICE,\n",
    "    'token': TOKEN,\n",
    "}\n",
    "\n",
    "# TOKENIZER = 'gpt2-xl'\n",
    "# TOKENIZER = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# TOKENIZER = 'meta-llama/Llama-2-7b-hf'\n",
    "TOKENIZER = 'google/gemma-7b'\n",
    "TOKENIZER_CONFIGS = {'token': TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9fd375bbdc43349ab40d7b7d3d00ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide a value for att_len!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m ContextCausalLMWrapper\u001b[39m.\u001b[39;49mfrom_pretrained(MODEL, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                                att_len\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                                                model_kwargs\u001b[39m=\u001b[39;49mMODEL_CONFIGS, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                                                tokenizer_name_or_path\u001b[39m=\u001b[39;49mTOKENIZER, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                                                tokenizer_kwargs\u001b[39m=\u001b[39;49mTOKENIZER_CONFIGS,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                                                )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39menable_benchmarking()\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:1293\u001b[0m, in \u001b[0;36mContextPreTrainedModelWrapper.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, att_len, model_args, model_kwargs, quantization_configs, lora_configs, peft, gradient_checkpointing, tokenizer_name_or_path, tokenizer_args, tokenizer_kwargs, **wrapper_kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     model \u001b[39m=\u001b[39m get_peft_model(model, lora_configs)\n\u001b[1;32m   1289\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m   1290\u001b[0m     tokenizer_name_or_path, \u001b[39m*\u001b[39mtokenizer_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_kwargs\n\u001b[1;32m   1291\u001b[0m )\n\u001b[0;32m-> 1293\u001b[0m wrapper \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(model, tokenizer, att_len\u001b[39m=\u001b[39;49matt_len)\n\u001b[1;32m   1295\u001b[0m \u001b[39mif\u001b[39;00m gradient_checkpointing:\n\u001b[1;32m   1296\u001b[0m     wrapper\u001b[39m.\u001b[39mgradient_checkpointing_enable()\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:1698\u001b[0m, in \u001b[0;36mContextCausalLMWrapper.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_head_attr: LMHeadAttr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lm_head_attr()\n\u001b[1;32m   1697\u001b[0m \u001b[39m# Wrappers\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformer_wrapper: Tuple[ContextTransformerWrapper] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformer_dtype(\n\u001b[1;32m   1699\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minternal_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformer_attr\u001b[39m.\u001b[39;49mvalue), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\n\u001b[1;32m   1700\u001b[0m ),\n\u001b[1;32m   1701\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_head_wrapper: Tuple[ContextLMHeadWrapper] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_head_dtype(\n\u001b[1;32m   1702\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minternal_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_head_attr\u001b[39m.\u001b[39mvalue), super_wrapper\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\n\u001b[1;32m   1703\u001b[0m ),\n\u001b[1;32m   1705\u001b[0m \u001b[39m# Lightning module parameters for fine-tuning\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:1413\u001b[0m, in \u001b[0;36mContextTransformerWrapper.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[39m# Wrappers\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_wrapper: Tuple \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_dtype(\n\u001b[1;32m   1407\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_attr\u001b[39m.\u001b[39mvalue),\n\u001b[1;32m   1408\u001b[0m     super_wrapper\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     ) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_position_embedding_attr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m ),\n\u001b[0;32m-> 1413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layers_wrapper: Tuple \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_layers_dtype(\n\u001b[1;32m   1414\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_layers_attr\u001b[39m.\u001b[39;49mvalue), super_wrapper\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\n\u001b[1;32m   1415\u001b[0m ),\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:970\u001b[0m, in \u001b[0;36mContextLayersWrapper.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_module, nn\u001b[39m.\u001b[39mModuleList):\n\u001b[1;32m    969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLayers must be encapsulated in a `ModuleList` object\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layer_wrappers: Tuple[nn\u001b[39m.\u001b[39mModuleList] \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mModuleList(\n\u001b[1;32m    971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_layer_dtype(layer, super_wrapper\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m layer \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_module\n\u001b[1;32m    972\u001b[0m ),\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/container.py:279\u001b[0m, in \u001b[0;36mModuleList.__init__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39;49m \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m modules\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/container.py:320\u001b[0m, in \u001b[0;36mModuleList.__iadd__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iadd__\u001b[39m(\u001b[39mself\u001b[39m, modules: Iterable[Module]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextend(modules)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/container.py:401\u001b[0m, in \u001b[0;36mModuleList.extend\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mModuleList.extend should be called with an \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39miterable, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mtype\u001b[39m(modules)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    400\u001b[0m offset \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 401\u001b[0m \u001b[39mfor\u001b[39;49;00m i, module \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(modules):\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(offset \u001b[39m+\u001b[39;49m i), module)\n\u001b[1;32m    403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:971\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_module, nn\u001b[39m.\u001b[39mModuleList):\n\u001b[1;32m    969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLayers must be encapsulated in a `ModuleList` object\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layer_wrappers: Tuple[nn\u001b[39m.\u001b[39mModuleList] \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_layer_dtype(layer, super_wrapper\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_module\n\u001b[1;32m    972\u001b[0m ),\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:738\u001b[0m, in \u001b[0;36mContextLayerWrapper.__init__\u001b[0;34m(self, layer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_forward_attr: LayerFeedForwardAttr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_feed_forward_attr()\n\u001b[1;32m    737\u001b[0m \u001b[39m# Wrappers\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attention_wrapper: Tuple \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attention_dtype(\n\u001b[1;32m    739\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_module, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attention_attr\u001b[39m.\u001b[39;49mvalue), super_wrapper\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\n\u001b[1;32m    740\u001b[0m ),\n\u001b[1;32m    741\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_forward_wrapper: Tuple \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_forward_dtype(\n\u001b[1;32m    742\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_module, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_forward_attr\u001b[39m.\u001b[39mvalue), super_wrapper\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\n\u001b[1;32m    743\u001b[0m ),\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:371\u001b[0m, in \u001b[0;36mContextAttentionWrapper.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_len \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39matt_len\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_len \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPlease provide a value for att_len!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    373\u001b[0m \u001b[39m# Init attention mask\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide a value for att_len!"
     ]
    }
   ],
   "source": [
    "model = ContextCausalLMWrapper.from_pretrained(MODEL, \n",
    "                                               att_len=2,\n",
    "                                               model_kwargs=MODEL_CONFIGS, \n",
    "                                               tokenizer_name_or_path=TOKENIZER, \n",
    "                                               tokenizer_kwargs=TOKENIZER_CONFIGS,\n",
    "                                               )\n",
    "model.enable_benchmarking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = 'The capital of Italy is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_wrapper = model(\n",
    "    **input_encodings,\n",
    "    return_dict=True,\n",
    "    output_attentions=True,\n",
    "    use_cache=True,\n",
    "    output_hidden_stsates=True,\n",
    "    return_attention_output=True,  # Self-attention layer output\n",
    "    return_feed_forward_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['att_len'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_encodings\u001b[39m.\u001b[39;49minput_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                          \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                         att_len\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                         do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                         max_length\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                         return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopen-brain/home/nicolobrunello/Documents/Projects/transformer_wrappers/notebooks/test_attention_mask.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                         output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:1854\u001b[0m, in \u001b[0;36mContextCausalLMWrapper.generate\u001b[0;34m(self, return_inner_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_wrapping:\n\u001b[1;32m   1852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1854\u001b[0m generate_output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1855\u001b[0m \u001b[39m# Re-run through layers to collect all data  # TODO find better solution\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m \u001b[39mif\u001b[39;00m return_inner_states \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_benchmarking:\n\u001b[1;32m   1857\u001b[0m     \u001b[39m#\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.12/site-packages/transformers/generation/utils.py:1689\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m generation_config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_generation_config(generation_config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1689\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1690\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_assistant(assistant_model)\n\u001b[1;32m   1692\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/transformer_wrappers/notebooks/../src/transformer_wrappers/wrappers/context.py:1848\u001b[0m, in \u001b[0;36mContextCausalLMWrapper._validate_model_kwargs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_model_kwargs\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49m_validate_model_kwargs(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.12/site-packages/transformers/generation/utils.py:1243\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1242\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1244\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['att_len'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_encodings.input_ids,\n",
    "                        do_sample=False, \n",
    "                        max_length=25, \n",
    "                        return_dict_in_generate=True, \n",
    "                        output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>The capital of Italy is capital is is is is is is is is is is is is is is is is is is'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(output['sequences'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6539e-04, 3.1738e-02,\n",
       "        5.8203e-01, 3.8477e-01], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.attentions[8][0][0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
