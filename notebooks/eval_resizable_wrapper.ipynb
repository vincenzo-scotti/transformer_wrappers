{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Wrapper Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from peft import AutoPeftModelForCausalLM, AutoPeftModelForSeq2SeqLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformer_wrappers.wrappers import ResizableCausalLMWrapper, CausalLMWrapper\n",
    "from transformer_wrappers.wrappers.resizable import ResizableTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_wrappers.data import OpenAssistantGuanaco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TOKEN = None  # HF Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXPERIMENTS_DIR_PATH: str = '/home/vincenzoscotti/Projects/transformer_wrappers/experiments'\n",
    "EXPERIMENT_SERIES_ID: str = 'resizable_wrapper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BENCHMARKS: List[str] = ['hellaswag', 'winogrande', 'truthfulqa_mc1', 'gsm8k', 'arc_challenge', 'mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE: int = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# MODEL = 'meta-llama/Llama-2-7b-hf'\n",
    "# MODEL = 'google/gemma-7b'\n",
    "MODEL_CONFIGS = {\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'device_map': 'auto',  # torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'token': TOKEN\n",
    "}\n",
    "QUANTIZATION_CONFIGS = {\n",
    "    'load_in_4bit': True,\n",
    "    'bnb_4bit_use_double_quant': True, \n",
    "    'bnb_4bit_quant_type': 'nf4', \n",
    "    'bnb_4bit_compute_dtype': torch.bfloat16\n",
    "}\n",
    "TOKENIZER = MODEL\n",
    "TOKENIZER_CONFIGS = {'token': TOKEN, 'pad_token': '</s>'}\n",
    "WRAPPER_CONFIGS_KEYS: List[str] = ['max_token_len']\n",
    "WRAPPER_CONFIGS_VALUES: List[str] = [[1, 2, 3, 4, 5, 6, 7, 8, None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_CONFIGS = {\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0.1,\n",
    "    'r': 16,\n",
    "    'bias': 'none',\n",
    "    'task_type': 'CAUSAL_LM'\n",
    "}\n",
    "TRAINING_ARGS = {\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'gradient_accumulation_steps': 32,\n",
    "    'per_device_eval_batch_size': 1,\n",
    "    'gradient_checkpointing': True,\n",
    "    'optim': 'paged_adamw_32bit',\n",
    "    'logging_steps': 50,\n",
    "    'report_to': 'tensorboard',\n",
    "    'save_strategy': 'epoch',\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'load_best_model_at_end': True,\n",
    "    'learning_rate': 2.e-5,\n",
    "    'bf16': True,\n",
    "    'tf32': True,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'warmup_ratio': 0.03,\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "    # 'disable_tqdm': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(EXPERIMENTS_DIR_PATH):\n",
    "    os.mkdir(EXPERIMENTS_DIR_PATH)\n",
    "if not os.path.exists(os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID)):\n",
    "    os.mkdir(os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID))\n",
    "\n",
    "current_experiments_dir = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID, MODEL.replace('/', '-'))\n",
    "if not os.path.exists(current_experiments_dir):\n",
    "    os.mkdir(current_experiments_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs_hash(configs):\n",
    "    return hashlib.sha256(str(configs).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results(**kwargs):\n",
    "    file_path = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID, 'results.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = pd.concat([df, pd.DataFrame.from_dict([kwargs])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame.from_dict([kwargs])\n",
    "    df.to_csv(file_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_df():\n",
    "    file_path = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID, 'results.csv')\n",
    "    return pd.read_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_results(results, configs, benchmark):\n",
    "    # Create dir (if necessary)\n",
    "    dir_path = os.path.join(current_experiments_dir, get_configs_hash(configs))\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    # Save configs (if necessary)\n",
    "    configs_file_path = os.path.join(dir_path, 'configs.yml')\n",
    "    if not os.path.exists(configs_file_path):\n",
    "        with open(configs_file_path, 'w') as f:\n",
    "            yaml.dump(configs, f)\n",
    "    # Save results\n",
    "    file_path = os.path.join(dir_path, f'{benchmark}.json')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump({k: results[k] for k in results if k!='config'}, f)\n",
    "    # Append results to main CSV\n",
    "    append_results(model=MODEL, benchmark=benchmark, **results['results'][benchmark], **configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_results_exist(configs, benchmark):\n",
    "    dir_path = os.path.join(current_experiments_dir, get_configs_hash(configs))\n",
    "    file_path = os.path.join(dir_path, f'{benchmark}.json')\n",
    "    \n",
    "    return os.path.exists(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_fine_tuning(configs):\n",
    "    # Create dir (if necessary)\n",
    "    dir_path = os.path.join(current_experiments_dir, f'FT-{get_configs_hash(configs)}')\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    istance_dir_path = os.path.join(dir_path, datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "    os.mkdir(istance_dir_path)\n",
    "    os.mkdir(os.path.join(istance_dir_path, 'logs'))\n",
    "    # Save configs (if necessary)\n",
    "    configs_file_path = os.path.join(istance_dir_path, 'configs.yml')\n",
    "    with open(configs_file_path, 'w') as f:\n",
    "        yaml.dump(\n",
    "            {\n",
    "                'wrapper_configs': configs, \n",
    "                'model': MODEL, \n",
    "                'model_configs': {k: v if k not in {'torch_dtype', 'device_map'} else str(v) for k, v in MODEL_CONFIGS.items()},\n",
    "                'quantization_configs': {k: v if k != 'bnb_4bit_compute_dtype' else str(v) for k, v in QUANTIZATION_CONFIGS.items()},\n",
    "                'tokenizer': TOKENIZER,\n",
    "                'tokenizer_configs': TOKENIZER_CONFIGS,\n",
    "                'lora_configs': LORA_CONFIGS,\n",
    "                'training_args': TRAINING_ARGS\n",
    "            }, \n",
    "            f\n",
    "        )\n",
    "\n",
    "    return istance_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fine_tuning_results(results, configs, benchmark, istance_dir_path):\n",
    "    # Save results\n",
    "    file_path = os.path.join(istance_dir_path, f'{benchmark}.json')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump({k: results[k] for k in results if k!='config'}, f)\n",
    "    # Append results to main CSV\n",
    "    append_results(model=MODEL, benchmark=benchmark, **results['results'][benchmark], **configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResizableCausalLMWrapper.from_pretrained(\n",
    "# model = CausalLMWrapper.from_pretrained(\n",
    "    MODEL, \n",
    "    model_kwargs=MODEL_CONFIGS,\n",
    "    tokenizer_kwargs=TOKENIZER_CONFIGS\n",
    ")\n",
    "model.enable_benchmarking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_configs: List[Dict] = [\n",
    "    {k: v for k, v in zip(WRAPPER_CONFIGS_KEYS, configs)}\n",
    "    for configs in itertools.product(*WRAPPER_CONFIGS_VALUES)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over benchmarks\n",
    "for benchmark in BENCHMARKS:\n",
    "    print(\"# Benchmark\")\n",
    "    # Iterate over configs\n",
    "    for configs in experiment_configs:\n",
    "        print(f\"## Configs: {configs}\")\n",
    "        # Run evaluation (if results are not avaialble yet)\n",
    "        if not check_results_exist(configs, benchmark):\n",
    "            # Set attribute values  # TODO fixme\n",
    "            for k, v in configs.items():\n",
    "                setattr(model, k, v)\n",
    "            # Run evaluation\n",
    "            results = lm_eval.simple_evaluate(\n",
    "                model=\"hf\",\n",
    "                model_args={'pretrained': model, 'tokenizer': model.tokenizer, 'backend': 'causal'},\n",
    "                # model_args='pretrained=mistralai/Mistral-7B-Instruct-v0.2,attn_implementation=eager,device_map=cuda',\n",
    "                tasks=[benchmark],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                log_samples=False,\n",
    "            )\n",
    "            # Save results\n",
    "            save_results(results, configs, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'max_token_len': 1, 'fine_tuning': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, **MODEL_CONFIGS, quantization_config=BitsAndBytesConfig(**QUANTIZATION_CONFIGS))\n",
    "tokenizer = ResizableTokenizer(AutoTokenizer.from_pretrained(TOKENIZER, **TOKENIZER_CONFIGS), max_token_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_configs = LoraConfig(**LORA_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = {\n",
    "    split: OpenAssistantGuanaco(split, tokenizer)\n",
    "    for split in ['train', 'validation', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_dir = setup_fine_tuning(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(fine_tuning_dir, 'logs'),\n",
    "    logging_dir=os.path.join(fine_tuning_dir, 'logs'),\n",
    "    **TRAINING_ARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=Dataset.from_dict({'text': data_splits['train'].data}),\n",
    "    eval_dataset=Dataset.from_dict({'text': data_splits['validation'].data}),\n",
    "    peft_config=lora_configs,\n",
    "    max_seq_length=4096,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    # data_collator=data_splits['train'].huggingface_collate\n",
    "    # formatting_func=lambda x: x['text']\n",
    "    dataset_text_field='text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_trainer.train()\n",
    "q_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_trainer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over benchmarks\n",
    "for benchmark in BENCHMARKS:\n",
    "    print(\"# Benchmark\")\n",
    "    # Run evaluation\n",
    "    results = lm_eval.simple_evaluate(\n",
    "        model=\"hf\",\n",
    "        model_args={'pretrained': model, 'tokenizer': tokenizer, 'backend': 'causal'},\n",
    "        # model_args='pretrained=mistralai/Mistral-7B-Instruct-v0.2,attn_implementation=eager,device_map=cuda',\n",
    "        tasks=[benchmark],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        log_samples=False,\n",
    "    )\n",
    "    # Save results\n",
    "    save_fine_tuning_results(results, configs, benchmark, fine_tuning_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Gather experiments results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df = load_results_df()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_acc_norm_mask = results_df['acc_norm,none'].isnull()\n",
    "null_acc_norm_mask\n",
    "results_df['accuracy'] = results_df['acc,none']\n",
    "results_df.loc[~null_acc_norm_mask, 'accuracy'] = results_df[~null_acc_norm_mask]['acc_norm,none'].values\n",
    "\n",
    "null_max_token_len_mask = results_df['max_token_len'].isnull()\n",
    "null_max_token_len_mask\n",
    "results_df.loc[null_max_token_len_mask, 'max_token_len'] = 5\n",
    "results_df['max_token_len'] = results_df['max_token_len'].astype(int)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "sns.relplot(\n",
    "    data=results_df,\n",
    "    # col='p_rate',\n",
    "    y='accuracy',\n",
    "    hue='benchmark',\n",
    "    x='max_token_len',\n",
    "    # size=...,\n",
    "    # hue='iterative',\n",
    "    style='model',\n",
    "    kind='line',\n",
    "    markers=True\n",
    "    # row=...\n",
    "    # xlim=[-0.1, 1.0],\n",
    "    # ylim=[0.0, 1.1]\n",
    ")\n",
    "plt.xlim([0.5, 5.5])\n",
    "plt.xticks(range(1, 6), [1,2,3,4,'Original'])\n",
    "plt.ylim([0.0, 1.0])\n",
    "# plt.hlines(0.8366859191396137, 0, 1, colors='g', linestyles='dashed', label='base_model')\n",
    "plt.grid()\n",
    "# plt.legend(loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
