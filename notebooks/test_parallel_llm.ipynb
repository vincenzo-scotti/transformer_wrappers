{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preliminary Investigations on Parallel Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/vincenzoscotti/Projects/transformer_wrappers/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincenzoscotti/anaconda3/envs/trwrap/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:55.357838Z",
     "start_time": "2024-02-16T09:20:55.177608Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformer_wrappers.wrappers import ParallelTransformerWrapper, ParallelCausalLMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:58.773348Z",
     "start_time": "2024-02-16T09:20:58.697516Z"
    }
   },
   "outputs": [],
   "source": [
    "COLOURS = [f'C{i}' for i in range(10)]\n",
    "STYLES = ['solid', 'dotted', 'dashed', 'dashdot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:59.872273Z",
     "start_time": "2024-02-16T09:20:59.658065Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = '...'  # HuggingFace token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:59.371162Z",
     "start_time": "2024-02-16T09:20:59.328004Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL = 'gpt2-xl'\n",
    "MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# MODEL = 'meta-llama/Llama-2-7b-hf'\n",
    "# MODEL = 'google/gemma-7b'\n",
    "MODEL_CONFIGS = {\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'attn_implementation': 'eager',\n",
    "    'device_map': DEVICE,\n",
    "    'token': TOKEN\n",
    "}\n",
    "\n",
    "# TOKENIZER = 'gpt2-xl'\n",
    "TOKENIZER = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# TOKENIZER = 'meta-llama/Llama-2-7b-hf'\n",
    "# TOKENIZER = 'google/gemma-7b'\n",
    "TOKENIZER_CONFIGS = {'token': TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:00.782467Z",
     "start_time": "2024-02-16T09:21:00.723050Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_string = 'The quick brown fox jumps over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.093517Z",
     "start_time": "2024-02-16T09:21:02.188380Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL, **MODEL_CONFIGS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER, **TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.149371Z",
     "start_time": "2024-02-16T09:21:06.097609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:09.302081Z",
     "start_time": "2024-02-16T09:21:06.131876Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model(\n",
    "    **input_encodings, \n",
    "    return_dict=True, \n",
    "    output_attentions=True, \n",
    "    use_cache=True, \n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ParallelTransformerWrapper.from_pretrained(MODEL, model_kwargs=MODEL_CONFIGS, tokenizer_name_or_path=TOKENIZER, tokenizer_kwargs=TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_wrapper = model(\n",
    "    **input_encodings,\n",
    "    return_dict=True,\n",
    "    output_attentions=True,\n",
    "    use_cache=True,\n",
    "    output_hidden_states=True,\n",
    "    return_attention_output=True,  # Self-attention layer output\n",
    "    return_feed_forward_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(output_wrapper, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_transformer.pkl', 'rb') as f:\n",
    "    output_wrapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_transformer.pkl', 'rb') as f:\n",
    "    output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(\n",
    "    output.last_hidden_state, output_wrapper['output_hidden_state']\n",
    "), '`last_hidden_state` not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output_hidden_state, output_wrapper_hidden_state) in enumerate(zip(\n",
    "        output.hidden_states, output_wrapper['hidden_states']\n",
    ")):\n",
    "    if i == 0:\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, output_wrapper_hidden_state\n",
    "        ), 'Initial embedding tensors not matching.'\n",
    "    if i == len(model.layers):\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, model.norm(output_wrapper_hidden_state)\n",
    "        ), f'`hidden_state` tensors at layer {i} not matching.'\n",
    "    else:\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, output_wrapper_hidden_state\n",
    "        ), f'`hidden_state` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (\n",
    "        output_hidden_state, prev_output_wrapper_hidden_state, attn_output_wrapper, ffnn_output_wrapper\n",
    ") in enumerate(zip(\n",
    "        output.hidden_states[1:],\n",
    "        output_wrapper['hidden_states'][:-1],\n",
    "        output_wrapper['attention_outputs'],\n",
    "        output_wrapper['feed_forward_outputs']\n",
    "), start=1):\n",
    "    output_wrapper_hidden_state = prev_output_wrapper_hidden_state + attn_output_wrapper + ffnn_output_wrapper\n",
    "    if i == len(model.layers):\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, model.norm(output_wrapper_hidden_state)\n",
    "        ), f'Composed `hidden_state` tensors at layer {i} not matching.'\n",
    "    else:\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, output_wrapper_hidden_state\n",
    "        ), f'Composed `hidden_state` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output_past_key_values, output_wrapper_past_key_values) in enumerate(zip(\n",
    "        output.past_key_values, output_wrapper['cache'],\n",
    "), start=1):\n",
    "    output_past_keys, output_past_values = output_past_key_values\n",
    "    output_wrapper_past_keys, output_wrapper_past_values = output_wrapper_past_key_values\n",
    "    assert torch.equal(\n",
    "        output_past_keys, output_wrapper_past_keys\n",
    "    ), f'`key` tensors at layer {i} not matching.'\n",
    "    assert torch.equal(\n",
    "        output_past_values, output_wrapper_past_values\n",
    "    ), f'`value` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output_attentions, output_wrapper_attentions) in enumerate(zip(\n",
    "        output.attentions, output_wrapper['attention_weights']\n",
    "), start=1):\n",
    "    assert torch.equal(\n",
    "        output_attentions, output_wrapper_attentions\n",
    "    ), f'`attentions` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.093517Z",
     "start_time": "2024-02-16T09:21:02.188380Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, **MODEL_CONFIGS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER, **TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.149371Z",
     "start_time": "2024-02-16T09:21:06.097609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:09.302081Z",
     "start_time": "2024-02-16T09:21:06.131876Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model.generate(input_encodings.input_ids, do_sample=False, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ParallelCausalLMWrapper.from_pretrained(MODEL, model_kwargs=MODEL_CONFIGS, tokenizer_name_or_path=TOKENIZER, tokenizer_kwargs=TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_wrapper = model.generate(input_encodings.input_ids, do_sample=False, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(output_wrapper, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_lm.pkl', 'rb') as f:\n",
    "    output_wrapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_lm.pkl', 'rb') as f:\n",
    "    output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(\n",
    "    output, output_wrapper['input_ids']\n",
    "), 'Generated sequences not matching.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ParallelCausalLMWrapper.from_pretrained(MODEL, model_kwargs=MODEL_CONFIGS, tokenizer_name_or_path=TOKENIZER, tokenizer_kwargs=TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(\"Q: Was Max Born the grandfather of Olivia Newton-John? A:\", return_tensors='pt').to(DEVICE)\n",
    "# input_encodings = model.tokenizer(\"Q: Son of an actor, this American guitarist and rock singer released many songs and albums and toured with his band. His name is \\\"Elvis\\\" what? A:\", return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  1 - Generated text: '<s> Q: Son of an actor, this American guitarist and rock singer released many songs and albums and toured with his band. His name is \"Elvis\" what? A: Presley is the correct answer. Elvis Presley was an American actor and musician, a major cultural icon of the 2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  2 - Generated text: '<s> Q: Son of an actor, this American guitarist and rock singer released many songs and albums and toured with his band. His name is \"Elvis\" what? A: The name is not \"Elvis\" - it\\'s \"Stevie\" - the correct answer is \"Stevie'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  4 - Generated text: '<s> Q: Son of an actor, this American guitarist and rock singer released many songs and albums and toured with his band. His name is \"Elvis\" what? A: Elvis\\n````\\n\\n``````````````````````````````````````'\n",
      "Prallelization rate:  8 - Generated text: '<s> Q: Son of an actor, this American guitarist and rock singer released many songs and albums and toured with his band. His name is \"Elvis\" what? A: helly, man, and thespand, . . . . . . . . . . . . . . . .'\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):  # len(model.transformer_wrapper.layers) + 1):\n",
    "    if len(model.transformer_wrapper.layers) % i == 0:\n",
    "        output_wrapper = model.generate(input_encodings.input_ids, rate=i, do_sample=False, max_length=64, recursive=True)\n",
    "        output_text = model.tokenizer.decode(output_wrapper['input_ids'][0])\n",
    "        print(f'Prallelization rate: {i:2d} - Generated text: {repr(output_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  1 - Generated text: '<s> Q: Was Max Born the grandfather of Olivia Newton-John? A: No, Max Born was not the grandfather of Olivia Newton-John. Max Born was a German-born, British-Jewish physicist, and Olivia Newton-John is an Australian-born singer and actress.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  2 - Generated text: '<s> Q: Was Max Born the grandfather of Olivia Newton-John? A: 1. S. A.\\n\\n1. A. D. A. A. D. A. A. D. A. A. D. A. A. D. A. A. D. A.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  4 - Generated text: '<s> Q: Was Max Born the grandfather of Olivia Newton-John? A:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'\n",
      "Prallelization rate:  8 - Generated text: '<s> Q: Was Max Born the grandfather of Olivia Newton-John? A: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #'\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):  # len(model.transformer_wrapper.layers) + 1):\n",
    "    if len(model.transformer_wrapper.layers) % i == 0:\n",
    "        output_wrapper = model.generate(input_encodings.input_ids, rate=i, do_sample=False, max_length=64, recursive=False)\n",
    "        output_text = model.tokenizer.decode(output_wrapper['input_ids'][0])\n",
    "        print(f'Prallelization rate: {i:2d} - Generated text: {repr(output_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(\"Q: Was Olivia Newton-John the granddaughter of Max Born? A:\", return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  1 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A: No, Olivia Newton-John is not the granddaughter of Max Born. Max Born was a German-born physicist, and Olivia Newton-John is an English-Australian singer and actress.</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  2 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A: No. The two are unrelated. The late Max Born was a German-British physicist, mathematician, and philosopher of science, best-known for his work in the field of quantum'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  4 - Generated text: \"<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A: O Ekwe-E, I meanwhile? A: Good dayn'nènkènènùkụnụnùkụnùkụnùkụnù\\n\\n\"\n",
      "Prallelization rate:  8 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A:Aheadousadaysaboutabanquabanquabakaard 20 20 20 20 20 20 20 20 20dayday  20 '\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):  # len(model.transformer_wrapper.layers) + 1):\n",
    "    if len(model.transformer_wrapper.layers) % i == 0:\n",
    "        output_wrapper = model.generate(input_encodings.input_ids, rate=i, do_sample=False, max_length=64, recursive=True)\n",
    "        output_text = model.tokenizer.decode(output_wrapper['input_ids'][0])\n",
    "        print(f'Prallelization rate: {i:2d} - Generated text: {repr(output_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  1 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A: No, Olivia Newton-John is not the granddaughter of Max Born. Max Born was a German-born physicist, and Olivia Newton-John is an English-Australian singer and actress.</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  2 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A: 1. S.\\n\\n1. A: 1. A: 1. A: 1. A: 1. A: 1. A: 1. A: 1. A'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prallelization rate:  4 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'\n",
      "Prallelization rate:  8 - Generated text: '<s> Q: Was Olivia Newton-John the granddaughter of Max Born? A: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #'\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):  # len(model.transformer_wrapper.layers) + 1):\n",
    "    if len(model.transformer_wrapper.layers) % i == 0:\n",
    "        output_wrapper = model.generate(input_encodings.input_ids, rate=i, do_sample=False, max_length=64, recursive=False)\n",
    "        output_text = model.tokenizer.decode(output_wrapper['input_ids'][0])\n",
    "        print(f'Prallelization rate: {i:2d} - Generated text: {repr(output_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
