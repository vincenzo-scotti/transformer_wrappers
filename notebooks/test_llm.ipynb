{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preliminary Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/vincenzoscotti/Projects/transformer_wrappers/src')\n",
    "# sys.path.append('/app/Projects/transformer_wrappers/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincenzoscotti/anaconda3/envs/trwrap/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:55.357838Z",
     "start_time": "2024-02-16T09:20:55.177608Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformer_wrappers.wrappers import TransformerWrapper, CausalLMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:58.773348Z",
     "start_time": "2024-02-16T09:20:58.697516Z"
    }
   },
   "outputs": [],
   "source": [
    "COLOURS = [f'C{i}' for i in range(10)]\n",
    "STYLES = ['solid', 'dotted', 'dashed', 'dashdot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:59.872273Z",
     "start_time": "2024-02-16T09:20:59.658065Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = '...'  # HuggingFace token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:20:59.371162Z",
     "start_time": "2024-02-16T09:20:59.328004Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL = 'gpt2-xl'\n",
    "# MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# MODEL = 'meta-llama/Llama-2-7b-hf'\n",
    "MODEL = 'google/gemma-7b'\n",
    "MODEL_CONFIGS = {\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'attn_implementation': 'eager',\n",
    "    'device_map': DEVICE,\n",
    "    'token': TOKEN\n",
    "}\n",
    "\n",
    "# TOKENIZER = 'gpt2-xl'\n",
    "# TOKENIZER = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# TOKENIZER = 'meta-llama/Llama-2-7b-hf'\n",
    "TOKENIZER = 'google/gemma-7b'\n",
    "TOKENIZER_CONFIGS = {'token': TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:00.782467Z",
     "start_time": "2024-02-16T09:21:00.723050Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_string = 'The quick brown fox jumps over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.093517Z",
     "start_time": "2024-02-16T09:21:02.188380Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL, **MODEL_CONFIGS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER, **TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.149371Z",
     "start_time": "2024-02-16T09:21:06.097609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:09.302081Z",
     "start_time": "2024-02-16T09:21:06.131876Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model(\n",
    "    **input_encodings, \n",
    "    return_dict=True, \n",
    "    output_attentions=True, \n",
    "    use_cache=True, \n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "model = TransformerWrapper.from_pretrained(MODEL, model_kwargs=MODEL_CONFIGS, tokenizer_name_or_path=TOKENIZER, tokenizer_kwargs=TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_wrapper = model(\n",
    "    **input_encodings,\n",
    "    return_dict=True,\n",
    "    output_attentions=True,\n",
    "    use_cache=True,\n",
    "    output_hidden_states=True,\n",
    "    return_attention_output=True,  # Self-attention layer output\n",
    "    return_feed_forward_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(output_wrapper, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_transformer.pkl', 'rb') as f:\n",
    "    output_wrapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_transformer.pkl', 'rb') as f:\n",
    "    output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(\n",
    "    output.last_hidden_state, output_wrapper['output_hidden_state']\n",
    "), '`last_hidden_state` not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output_hidden_state, output_wrapper_hidden_state) in enumerate(zip(\n",
    "        output.hidden_states, output_wrapper['hidden_states']\n",
    ")):\n",
    "    if i == 0:\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, output_wrapper_hidden_state\n",
    "        ), 'Initial embedding tensors not matching.'\n",
    "    if i == len(model.layers):\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, model.norm(output_wrapper_hidden_state)\n",
    "        ), f'`hidden_state` tensors at layer {i} not matching.'\n",
    "    else:\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, output_wrapper_hidden_state\n",
    "        ), f'`hidden_state` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (\n",
    "        output_hidden_state, prev_output_wrapper_hidden_state, attn_output_wrapper, ffnn_output_wrapper\n",
    ") in enumerate(zip(\n",
    "        output.hidden_states[1:],\n",
    "        output_wrapper['hidden_states'][:-1],\n",
    "        output_wrapper['attention_outputs'],\n",
    "        output_wrapper['feed_forward_outputs']\n",
    "), start=1):\n",
    "    output_wrapper_hidden_state = prev_output_wrapper_hidden_state + attn_output_wrapper + ffnn_output_wrapper\n",
    "    if i == len(model.layers):\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, model.norm(output_wrapper_hidden_state)\n",
    "        ), f'Composed `hidden_state` tensors at layer {i} not matching.'\n",
    "    else:\n",
    "        assert torch.equal(\n",
    "            output_hidden_state, output_wrapper_hidden_state\n",
    "        ), f'Composed `hidden_state` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output_past_key_values, output_wrapper_past_key_values) in enumerate(zip(\n",
    "        output.past_key_values, output_wrapper['cache'],\n",
    "), start=1):\n",
    "    output_past_keys, output_past_values = output_past_key_values\n",
    "    output_wrapper_past_keys, output_wrapper_past_values = output_wrapper_past_key_values\n",
    "    assert torch.equal(\n",
    "        output_past_keys, output_wrapper_past_keys\n",
    "    ), f'`key` tensors at layer {i} not matching.'\n",
    "    assert torch.equal(\n",
    "        output_past_values, output_wrapper_past_values\n",
    "    ), f'`value` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output_attentions, output_wrapper_attentions) in enumerate(zip(\n",
    "        output.attentions, output_wrapper['attention_weights']\n",
    "), start=1):\n",
    "    assert torch.equal(\n",
    "        output_attentions, output_wrapper_attentions\n",
    "    ), f'`attentions` tensors at layer {i} not matching.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.093517Z",
     "start_time": "2024-02-16T09:21:02.188380Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, **MODEL_CONFIGS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER, **TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:06.149371Z",
     "start_time": "2024-02-16T09:21:06.097609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T09:21:09.302081Z",
     "start_time": "2024-02-16T09:21:06.131876Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model.generate(input_encodings.input_ids, do_sample=False, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "model = CausalLMWrapper.from_pretrained(MODEL, model_kwargs=MODEL_CONFIGS, tokenizer_name_or_path=TOKENIZER, tokenizer_kwargs=TOKENIZER_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(input_string, return_tensors='pt').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m output_wrapper \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_encodings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/transformer_wrappers/src/transformer_wrappers/wrappers/base.py:1244\u001B[0m, in \u001B[0;36mCausalLMWrapper.generate\u001B[0;34m(self, return_inner_states, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1243\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, return_inner_states: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 1244\u001B[0m     generate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1245\u001B[0m     \u001B[38;5;66;03m# Re-run through layers to collect all data  # TODO find better solution\u001B[39;00m\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_inner_states:\n\u001B[1;32m   1247\u001B[0m         \u001B[38;5;66;03m#\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/trwrap/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/trwrap/lib/python3.12/site-packages/transformers/generation/utils.py:1544\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1526\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n\u001B[1;32m   1527\u001B[0m         input_ids,\n\u001B[1;32m   1528\u001B[0m         candidate_generator\u001B[38;5;241m=\u001B[39mcandidate_generator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1541\u001B[0m     )\n\u001B[1;32m   1542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n\u001B[1;32m   1543\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[0;32m-> 1544\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgreedy_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1545\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1546\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1547\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1549\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1550\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1551\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_logits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1553\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1554\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1555\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1556\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n\u001B[1;32m   1559\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/anaconda3/envs/trwrap/lib/python3.12/site-packages/transformers/generation/utils.py:2401\u001B[0m, in \u001B[0;36mGenerationMixin.greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   2398\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   2400\u001B[0m \u001B[38;5;66;03m# prepare model inputs\u001B[39;00m\n\u001B[0;32m-> 2401\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_inputs_for_generation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2403\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[1;32m   2404\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\n\u001B[1;32m   2405\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[1;32m   2406\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   2407\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   2408\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   2409\u001B[0m )\n",
      "File \u001B[0;32m~/Projects/transformer_wrappers/src/transformer_wrappers/wrappers/base.py:1276\u001B[0m, in \u001B[0;36mCausalLMWrapper.prepare_inputs_for_generation\u001B[0;34m(self, base_model_output, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_wrapper()\n\u001B[1;32m   1266\u001B[0m \u001B[38;5;66;03m# TODO find actual solution\u001B[39;00m\n\u001B[1;32m   1267\u001B[0m \u001B[38;5;66;03m# if kwargs.get(POSITION_IDS) is None and isinstance(self.base_model, LlamaPreTrainedModel):\u001B[39;00m\n\u001B[1;32m   1268\u001B[0m \u001B[38;5;66;03m#     seq_length = kwargs.get(INPUT_IDS, kwargs.get(INPUT_EMBEDS, args[0])).size(1)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;66;03m#     kwargs[POSITION_IDS] = position_ids\u001B[39;00m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m-> 1276\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_inputs_for_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1277\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   1278\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m|\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase_model_output\u001B[39m\u001B[38;5;124m'\u001B[39m: base_model_output}\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "File \u001B[0;32m~/anaconda3/envs/trwrap/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py:1167\u001B[0m, in \u001B[0;36mGemmaForCausalLM.prepare_inputs_for_generation\u001B[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001B[0m\n\u001B[1;32m   1163\u001B[0m     position_ids \u001B[38;5;241m=\u001B[39m position_ids[:, past_length:]\n\u001B[1;32m   1165\u001B[0m \u001B[38;5;66;03m# TODO @gante we should only keep a `cache_position` in generate, and do +=1.\u001B[39;00m\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;66;03m# same goes for position ids. Could also help with continued generation.\u001B[39;00m\n\u001B[0;32m-> 1167\u001B[0m cache_position \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(past_length, past_length \u001B[38;5;241m+\u001B[39m \u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], device\u001B[38;5;241m=\u001B[39mposition_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;66;03m# if `inputs_embeds` are passed, we only want to use them in the 1st generation step\u001B[39;00m\n\u001B[1;32m   1170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output_wrapper = model.generate(input_encodings.input_ids, do_sample=False, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(output_wrapper, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_wrapper_lm.pkl', 'rb') as f:\n",
    "    output_wrapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.tmp_output_lm.pkl', 'rb') as f:\n",
    "    output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(\n",
    "    output, output_wrapper['input_ids']\n",
    "), 'Generated sequences not matching.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
