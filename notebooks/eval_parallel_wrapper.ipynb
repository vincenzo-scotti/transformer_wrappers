{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Wrapper Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/Projects/transformer_wrappers/src')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformer_wrappers.wrappers import ParallelCausalLMWrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import lm_eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants and globals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TOKEN = None  # HF Token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EXPERIMENTS_DIR_PATH: str = '~/Projects/transformer_wrappers/experiments'\n",
    "EXPERIMENT_SERIES_ID: str = 'parallel_wrapper'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BENCHMARKS: List[str] = ['hellaswag']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE: int = 8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'  \n",
    "# MODEL = 'meta-llama/Llama-2-7b-hf'\n",
    "# MODEL = 'google/gemma-7b'\n",
    "MODEL_CONFIGS = {\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'device_map': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'token': TOKEN\n",
    "    # 'load_in_4bit': True, \n",
    "    # 'bnb_4bit_use_double_quant': True, \n",
    "    # 'bnb_4bit_quant_type': 'nf4', \n",
    "    # 'bnb_4bit_compute_dtype': torch.bfloat16\n",
    "}\n",
    "TOKENIZER = MODEL\n",
    "TOKENIZER_CONFIGS = {'token': TOKEN}\n",
    "WRAPPER_CONFIGS_KEYS: List[str] = ['p_rate', 'block_parallel', 'iterative', 'scaling']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "experiment_configs: List[Dict] = [\n",
    "    {k: v for k, v in zip(WRAPPER_CONFIGS_KEYS, configs)}\n",
    "    for configs in itertools.product([2, 4], [True, False], [True, False], [True, False])\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_res(results, model, config):\n",
    "    dir_path = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID)\n",
    "    file_path = os.path.join(dir_path, f'{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.json')\n",
    "    if not os.path.exists(EXPERIMENTS_DIR_PATH):\n",
    "        os.mkdir(EXPERIMENTS_DIR_PATH)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(\n",
    "            {k: results[k] for k in results if k!='config'} | {'model': model, 'config': config}, f\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_res():\n",
    "    experiment_results = list()\n",
    "    dir_path = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID)\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(dir_path, file_name), 'r') as f:\n",
    "                results = json.load(f)['results']\n",
    "            experiment_results.append(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_res_exists(model, config):\n",
    "    dir_path = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID)\n",
    "    if not os.path.exists(EXPERIMENTS_DIR_PATH):\n",
    "        return False\n",
    "    if not os.path.exists(dir_path):\n",
    "        return False\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(dir_path, file_name), 'r') as f:\n",
    "                results = json.load(f)\n",
    "            if (\n",
    "                results['model'] == model and \n",
    "                results['config'] == config and \n",
    "                set(BENCHMARKS) == set(results['results'].keys())\n",
    "            ):\n",
    "                return True\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_res_df():\n",
    "    dir_path = os.path.join(EXPERIMENTS_DIR_PATH, EXPERIMENT_SERIES_ID)\n",
    "    data = []\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(dir_path, file_name), 'r') as f:\n",
    "                results = json.load(f)\n",
    "            for benchmark in results['results']:\n",
    "                data.append({\n",
    "                    'benchmark': benchmark, \n",
    "                    'score': results[benchmark]['acc_norm,none'], \n",
    "                    'model': results['model'], \n",
    "                    **results['config']\n",
    "                })\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1:\n",
    "\n",
    "Change the attention mask to reduce the self-attention layers to \"see\" only 0, 1, 2, 4, 8, ... 64, 128, 256, ..., 8k tokens into the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = ParallelCausalLMWrapper.from_pretrained(\n",
    "    MODEL, \n",
    "    model_kwargs=MODEL_CONFIGS,\n",
    "    tokenizer_kwargs=TOKENIZER_CONFIGS\n",
    ")\n",
    "model.enable_benchmarking()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for config in experiment_configs:\n",
    "    # \n",
    "    if not check_res_exists(MODEL, config):\n",
    "        # Set attn len\n",
    "        for k, v in config.items():\n",
    "            setattr(model.transformer_wrapper, k, v)\n",
    "        # Run evaluation\n",
    "        results = lm_eval.simple_evaluate(\n",
    "            model=\"hf\",\n",
    "            model_args={'pretrained': model, 'tokenizer': model.tokenizer, 'backend': 'causal'},\n",
    "            # model_args='pretrained=mistralai/Mistral-7B-Instruct-v0.2,attn_implementation=eager,device_map=cuda',\n",
    "            tasks=BENCHMARKS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            log_samples=True,\n",
    "        )\n",
    "        # Save results\n",
    "        save_res(results, MODEL, config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gather experiments results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df = load_res_df()\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df.to_csv(os.path.join(EXPERIMENTS_DIR_PATH, f'results_{datetime.now()}.csv'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualise results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "sns.lineplot(  # TODO\n",
    "    data=results_df, \n",
    "    x='Attention size', \n",
    "    y='score', \n",
    "    hue='model', \n",
    "    col='benchmark'\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.xlim([0, 1024])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig.savefig(os.path.join(EXPERIMENTS_DIR_PATH, f'results_attn.pdf'))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
