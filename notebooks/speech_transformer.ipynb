{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Speech transformer "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42dd89abb631d1e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b7de2f2910c7a77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36658f319f21ed06"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/Projects/transformer_wrappers/src')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T15:34:11.667016Z",
     "start_time": "2024-10-10T15:34:11.665158Z"
    }
   },
   "id": "3ce5db8a5a94b70f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:27.208852Z",
     "start_time": "2024-10-14T08:18:26.578430Z"
    }
   },
   "id": "dad5c72c42686d67",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincenzoscotti/anaconda3/envs/trwrap/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:27.760323Z",
     "start_time": "2024-10-14T08:18:27.209769Z"
    }
   },
   "id": "1da0d0b936e4c968",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformer_wrappers.wrappers import SpeechCausalLMWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:28.312878Z",
     "start_time": "2024-10-14T08:18:27.760848Z"
    }
   },
   "id": "2baf878f3b022d7a",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants and globals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67faf2489a3d75be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TOKEN = None  # HF Token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:28.315201Z",
     "start_time": "2024-10-14T08:18:28.313660Z"
    }
   },
   "id": "f69813390cefb01",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL = 'gpt2'  \n",
    "# MODEL = 'mistralai/Mistral-7B-Instruct-v0.3'  \n",
    "# MODEL = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "# MODEL = 'google/gemma-2-9b-it'\n",
    "MODEL_CONFIGS = {\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'device_map': 'cpu',  # torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'token': TOKEN\n",
    "}\n",
    "TOKENIZER_CONFIGS = {'token': TOKEN, 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|audio|>']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:28.317804Z",
     "start_time": "2024-10-14T08:18:28.315523Z"
    }
   },
   "id": "23bd06e783d5cb",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "QUANTIZATION_CONFIGS = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type='nf4', \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:28.320249Z",
     "start_time": "2024-10-14T08:18:28.318122Z"
    }
   },
   "id": "b0632824bc83333b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LORA_CONFIGS = LoraConfig(\n",
    "    target_modules='all-linear',\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:28.322204Z",
     "start_time": "2024-10-14T08:18:28.320563Z"
    }
   },
   "id": "6dbebabb7e185b67",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincenzoscotti/anaconda3/envs/trwrap/lib/python3.12/site-packages/peft/tuners/lora/layer.py:861: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/vincenzoscotti/anaconda3/envs/trwrap/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SpeechCausalLMWrapper.from_pretrained(\n",
    "    MODEL,\n",
    "    model_kwargs=MODEL_CONFIGS,\n",
    "    # quantization_configs=QUANTIZATION_CONFIGS,\n",
    "    lora_configs=LORA_CONFIGS,\n",
    "    tokenizer_kwargs=TOKENIZER_CONFIGS\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:29.816137Z",
     "start_time": "2024-10-14T08:18:28.322509Z"
    }
   },
   "id": "3e6f4622200bc799",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9b56abef0327b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Ellipsis"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:29.819550Z",
     "start_time": "2024-10-14T08:18:29.817090Z"
    }
   },
   "id": "8f16202dca51ec68",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Speech recognition\n",
    "\n",
    "In this first example we show how to forward an input composed of text and audio to the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7dcbd053f3daf29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = f'Transcribe the following audio clip:\\n{model.audio_token}\\n\\nTranscription:\\n\"In a hole in the ground there lived a hobbit.\"'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:29.821721Z",
     "start_time": "2024-10-14T08:18:29.820243Z"
    }
   },
   "id": "caf8525c5ed62f05",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "audio_file_path = '../audio.wav'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:31.870333Z",
     "start_time": "2024-10-14T08:18:31.868635Z"
    }
   },
   "id": "bd2427db585594c9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_encoding = model.prepare_input(text, audio_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:32.624784Z",
     "start_time": "2024-10-14T08:18:32.254574Z"
    }
   },
   "id": "17955e75a9bd184",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 402])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_encoding['input_spectrograms'][0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:32.633446Z",
     "start_time": "2024-10-14T08:18:32.629975Z"
    }
   },
   "id": "9e321bf389a8b2d7",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output = model.forward(**input_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:33.475878Z",
     "start_time": "2024-10-14T08:18:33.358709Z"
    }
   },
   "id": "408d2726c9989a5a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_output = model.prepare_output(text, audio_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:33.838022Z",
     "start_time": "2024-10-14T08:18:33.827904Z"
    }
   },
   "id": "882c13cf8d57566c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 402])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_output['target_spectrograms'][0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:34.745004Z",
     "start_time": "2024-10-14T08:18:34.742510Z"
    }
   },
   "id": "402211a5f8f748e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(13674.4873, grad_fn=<AddBackward0>),\n {'language_modelling_loss': tensor(24.9689, grad_fn=<NllLossBackward0>),\n  'spectrogram_generation_loss': tensor(13649.5186, grad_fn=<MeanBackward0>)})"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model._loss(\n",
    "    token_logits=output['logits'],\n",
    "    token_labels=target_output['token_labels'],\n",
    "    predicted_spectrograms=output['spectrograms'],\n",
    "    target_spectrograms=target_output['target_spectrograms']\n",
    ")\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:18:35.415462Z",
     "start_time": "2024-10-14T08:18:35.399055Z"
    }
   },
   "id": "bb26fd7feb4ebfaa",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9c7d728544919462"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
