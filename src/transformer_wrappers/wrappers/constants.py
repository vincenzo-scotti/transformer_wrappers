# Base params

INPUT_IDS: str = 'input_ids'
INPUT_EMBEDS: str = 'inputs_embeds'
ATTENTION_MASK: str = 'attention_mask'
POSITION_IDS: str = 'position_ids'
PAST_KEY_VALUES: str = 'past_key_values'
CACHE_POSITION: str = 'cache_position'

LAYER_PAST: str = 'layer_past'
PAST_KEY_VALUE: str = 'past_key_value'

HIDDEN_STATES: str = 'hidden_states'
LAST_HIDDEN_STATE: str = 'last_hidden_state'

LOGITS: str = 'logits'

LOSS: str = 'loss'

USE_CACHE: str = 'use_cache'
OUTPUT_ATTENTIONS: str = 'output_attentions'
OUTPUT_HIDDEN_STATES: str = 'output_hidden_states'
RETURN_DICT: str = 'return_dict'

# Additional Params

EMBEDDINGS: str = 'embeddings'
VALID_MASK: str = 'valid_mask'

BATCH_SIZE: str = 'batch_size'
PREFIX_LENGTH: str = 'prefix_length'
SEQ_LENGTH: str = 'sequence_length'
DTYPE: str = 'dtype'
DEVICE: str = 'device'

ATTN_PARAMS: str = 'attention_params'

INPUT_HIDDEN_STATE: str = 'input_hidden_state'
CURR_HIDDEN_STATE: str = 'current_hidden_state'
CURR_KEY_VALUE: str = 'current_key_values'
CURR_ATTN_WEIGHTS: str = 'current_attention_weights'

INTERMEDIATE_HIDDEN_STATE: str = 'intermediate_hidden_state'
ATTN_OUTPUT: str = 'attention_output'
FFNN_OUTPUT: str = 'feed_forward_output'

CACHE: str = 'cache'
INTERMEDIATE_HIDDEN_STATES: str = 'intermediate_hidden_states'
ATTN_WEIGHTS: str = 'attention_weights'
OUT_HIDDEN_STATE: str = 'output_hidden_state'

ATTN_OUTPUTS: str = 'attention_outputs'
FFNN_OUTPUTS: str = 'feed_forward_outputs'

ADD_ATTN_RESIDUAL: str = 'add_attn_residual'
ADD_FFNN_RESIDUAL: str = 'add_ffnn_residual'

RETURN_INTERMEDIATE_HIDDEN_STATES: str = 'return_intermediate_hidden_state'
RETURN_ATTENTION_OUTPUT: str = 'return_attention_output'
RETURN_FFNN_OUTPUT: str = 'return_feed_forward_output'

BASE_MODEL: str = 'base_model'

ITERATION: str = 'iteration'

OUTPUT_IDS: str = 'output_ids'
