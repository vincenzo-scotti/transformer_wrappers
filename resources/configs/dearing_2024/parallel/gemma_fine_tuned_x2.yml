# Main
experiment_series: DEARING_2024  # <wrapper_dtype>
experiment_id: gemma_7b__parallel__parallel__fine_tuned__x2
experiments_dir_path: ./experiments/
hf_token: &hf_token hf_
random_seed: 42
log_file: true
log_level: INFO
# LM evaluation harness configs
lm_eval:
  tasks:
    - hellaswag
    - winogrande
    - truthfulqa_mc1
    - gsm8k
    - arc_challenge
    - mmlu
  batch_size: 2
  log_samples: true
  random_seed: &random_seed 42
  numpy_random_seed: *random_seed
  torch_random_seed: *random_seed
  fewshot_random_seed: *random_seed
# Model
model:
  # Task
  dtype: ParallelCausalLMWrapper  # CausalLMWrapper | ParallelCausalLMWrapper | ResizableCausalLMWrapper
  # Model
  pretrained_model_name_or_path: ./resources/models/gemma_7b__parallel_x2__fine_tuning__2024_06_25_09_43_26  # mistralai/Mistral-7B-v0.3 | google/gemma-7b | meta-llama/Meta-Llama-3-8B
  # Model kwargs
  model_kwargs:
    device_map: cuda
    token: *hf_token
  # Quantization configs
  quantization_configs:
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16
  # PEFT model
  peft: true
  # Tokenizer
  tokenizer_name_or_path: google/gemma-7b
  # Tokenizer kwargs
  tokenizer_kwargs:
    token: *hf_token
  # Wrapper specific parameters
  ## Parallel
  p_rate: 2
  block_parallel: true
  iterative: true
  scaling: false
# Experimental configs
exp:
  # params: {}
  # param_groups: []
  overwrite: false  # Whether to repeat experiments with results that already exist
