# Main
experiment_series: <series>  # <wrapper_dtype>_<data_set>
experiment_id: <model>
experiments_dir_path: ./experiments/
hf_token: &hf_token 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
random_seed: 42
log_file: true
log_level: INFO
# Data
data:
  corpus: <corpus_dtype>  # OpenAssistantGuanaco
  splits:
    - train
    - validation
    - test
  params:
    {}
    # path: ./resources/data/raw/<data_path>  # ...
# Callbacks
callbacks:
  early_stopping:
    monitor: &estop_monitor Loss/Validation
    mode: &estop_mode min
    patience: 5  # Early stopping patience
    min_delta: 1.e-3  # Minimum metric variation
    verbose: true
  checkpoint_callback:
    monitor: *estop_monitor
    mode: *estop_mode
    # save_top_k: 1
# Model
model:
  # Task
  dtype: <wrapper_dtype>  # CausalLMWrapper | ParallelCausalLMWrapper | ResizableCausalLMWrapper
  # Model
  pretrained_model_name_or_path: <model_path>  # mistralai/Mistral-7B-Instruct-v0.2 | google/gemma-7b | meta-llama/Llama-2-7b-hf
  # Model kwargs
  model_kwargs:
    torch_dtype: &torch_dtype torch.bfloat16
    device_map: cuda  # torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    token: *hf_token
  # Quantization configs
  quantization_configs:
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: *torch_dtype
  # LoRA configs
  lora_config:
    target_modules: all-linear
    lora_alpha: 16
    lora_dropout: 0.1
    r: 16
    bias: none
    task_type: CAUSAL_LM
  # Tokenizer kwargs
  tokenizer_kwargs:
    token: *hf_token
    pad_token: </s>
  # Wrapper specific parameters
  ## Parallel
  # p_rate: 3
  # block_parallel: true
  # iterative: true
  ## Character
  max_token_len: 1
# Model hyperparameters
hyperparameters:
  # Optimiser
  optimiser_params:
    dtype: AdamW  # AdamW | RMSprop
    lr: 6.25e-5  # Learning rate
    # scale_parameter: false  # Used by AdaFactor
    # relative_step: false  # Used by AdaFactor
  # Trainer
  trainer_params:
    accumulate_grad_batches: 1
    precision: bf16  # Mixed precision (bf16-mixed or 16-mixed)
    gradient_clip_val: 1.0  # Gradient clipping
    max_epochs: 3  # Number of training epochs
  # Data loader
  data_loader_params:
    train:
      batch_size: 1
      num_workers: &workers 4
    validation:
      batch_size: &eval_bs 1
      num_workers: *workers
    test:
      batch_size: *eval_bs
      num_workers: *workers
